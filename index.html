<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>YOLO Object Detection</title>
  <style>
    body { margin: 0; overflow: hidden; }
    #video, #overlay {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    #switch-camera-btn {
      position: absolute;
      bottom: 20px;
      left: 50%;
      transform: translateX(-50%);
      padding: 12px 24px;
      font-size: 16px;
      cursor: pointer;
      z-index: 10;
      border-radius: 8px;
      border: 1px solid #ccc;
      background-color: #fff;
    }
  </style>
</head>
<body>
  <video id="video" autoplay playsinline muted></video>
  <canvas id="overlay"></canvas>
  <button id="switch-camera-btn">Switch Camera</button>
  <div id="fps-counter" style="
    position: absolute;
    top: 10px;
    left: 10px;
    color: #fff;
    background: rgba(0,0,0,0.5);
    padding: 4px 10px;
    border-radius: 6px;
    font-family: monospace;
    font-size: 18px;
    z-index: 20;
  ">FPS: 0</div>

  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <script>
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const ctx = overlay.getContext('2d');
    let session;
    let currentStream;
    let currentFacingMode = 'user'; // 'user' for front, 'environment' for rear
    // Set your model's input size (YOLOv8s default)
    const modelWidth = 640;
    const modelHeight = 640;

    // COCO class labels
    const cocoLabels = [
      'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
      'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
      'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
      'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',
      'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
      'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
      'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
      'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',
      'hair drier', 'toothbrush'
    ];

    let frameCount = 0;
    let lastFpsUpdate = performance.now();
    let fps = 0;

    function updateFpsCounter() {
      const now = performance.now();
      if (now - lastFpsUpdate >= 1000) {
        fps = frameCount;
        frameCount = 0;
        lastFpsUpdate = now;
        document.getElementById('fps-counter').textContent = `FPS: ${fps}`;
      }
    }

    async function setupCamera() {
      if (currentStream) {
        currentStream.getTracks().forEach(track => track.stop());
      }

      const constraints = { video: { facingMode: currentFacingMode } };

      try {
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        currentStream = stream;
        await video.play();
        overlay.width = video.videoWidth;
        overlay.height = video.videoHeight;
      } catch (error) {
          console.error(`Error accessing camera with facingMode ${currentFacingMode}:`, error);
          alert(`Could not switch to ${currentFacingMode} camera. Please ensure it is available and permissions are granted.`);
      }
    }

    function preprocess(frame) {
      // Draw frame to an offscreen canvas of model dimensions
      const canvas = document.createElement('canvas');
      canvas.width = modelWidth;
      canvas.height = modelHeight;
      const offCtx = canvas.getContext('2d');
      offCtx.drawImage(frame, 0, 0, modelWidth, modelHeight);
      const imageData = offCtx.getImageData(0, 0, modelWidth, modelHeight).data;
      const data = new Float32Array(1 * 3 * modelHeight * modelWidth);
      // NHWC to NCHW with normalization
      for (let i = 0; i < modelWidth * modelHeight; i++) {
        data[i] = imageData[i * 4] / 255.0;
        data[i + modelWidth * modelHeight] = imageData[i * 4 + 1] / 255.0;
        data[i + 2 * modelWidth * modelHeight] = imageData[i * 4 + 2] / 255.0;
      }
      return new ort.Tensor('float32', data, [1, 3, modelHeight, modelWidth]);
    }

    function drawDetections(detections) {
      ctx.clearRect(0, 0, overlay.width, overlay.height);
      ctx.lineWidth = 2;
      ctx.font = '16px sans-serif';
      detections.forEach(det => {
        const [x, y, w, h] = det.bbox;
        const label = cocoLabels[det.class];
        ctx.strokeStyle = 'red';
        ctx.strokeRect(
          x * overlay.width,
          y * overlay.height,
          w * overlay.width,
          h * overlay.height
        );
        ctx.fillStyle = 'red';
        ctx.fillText(
          `${label} (${(det.score * 100).toFixed(1)}%)`,
          x * overlay.width,
          y * overlay.height - 5
        );
      });
    }

    function intersectionOverUnion(boxA, boxB) {
      const xA = Math.max(boxA[0], boxB[0]);
      const yA = Math.max(boxA[1], boxB[1]);
      const xB = Math.min(boxA[2], boxB[2]);
      const yB = Math.min(boxA[3], boxB[3]);

      const interArea = Math.max(0, xB - xA) * Math.max(0, yB - yA);

      const boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1]);
      const boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1]);

      const iou = interArea / (boxAArea + boxBArea - interArea);
      return iou;
    }

    function nonMaximumSuppression(boxes, iouThreshold) {
      boxes.sort((a, b) => b.score - a.score);
      const result = [];
      while (boxes.length > 0) {
        result.push(boxes[0]);
        boxes = boxes.slice(1).filter(box => {
            const iou = intersectionOverUnion(boxes[0].box, box.box);
            return iou < iouThreshold;
        });
      }
      return result.map(box => ({
        class: box.classId,
        score: box.score,
        bbox: [
          box.box[0] / modelWidth,
          box.box[1] / modelHeight,
          (box.box[2] - box.box[0]) / modelWidth,
          (box.box[3] - box.box[1]) / modelHeight
        ]
      }));
    }

    function postprocess(output, confThreshold = 0.25, iouThreshold = 0.45) {
      const data = output.data;
      const numClasses = cocoLabels.length;
      const numBoxes = output.dims[2];

      const boxes = [];
      for (let i = 0; i < numBoxes; i++) {
        let maxScore = -1;
        let classId = -1;

        for (let j = 0; j < numClasses; j++) {
          const score = data[(4 + j) * numBoxes + i];
          if (score > maxScore) {
            maxScore = score;
            classId = j;
          }
        }

        if (maxScore > confThreshold) {
          const cx = data[0 * numBoxes + i];
          const cy = data[1 * numBoxes + i];
          const w = data[2 * numBoxes + i];
          const h = data[3 * numBoxes + i];
          const x1 = cx - w / 2;
          const y1 = cy - h / 2;
          const x2 = cx + w / 2;
          const y2 = cy + h / 2;
          boxes.push({ classId: classId, score: maxScore, box: [x1, y1, x2, y2] });
        }
      }

      return nonMaximumSuppression(boxes, iouThreshold);
    }

    async function detectLoop() {
      frameCount++;
      updateFpsCounter();
      const inputTensor = preprocess(video);
      const feeds = { [session.inputNames[0]]: inputTensor };
      const outputMap = await session.run(feeds);
      const outputTensor = outputMap[session.outputNames[0]];
      const detections = postprocess(outputTensor);
      drawDetections(detections);
      requestAnimationFrame(detectLoop);
    }

    async function init() {
      document.getElementById('switch-camera-btn').addEventListener('click', () => {
        currentFacingMode = (currentFacingMode === 'user') ? 'environment' : 'user';
        setupCamera();
      });

      await setupCamera();
      console.log('20250624-2005')
      
      // Load your YOLOv8s ONNX model (e.g. /models/yolov8s.onnx)
      session = await ort.InferenceSession.create('models/yolov8s.onnx');
      detectLoop();
    }

    init();
  </script>
</body>
</html>
